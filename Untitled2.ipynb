{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef601ee-4743-4b84-8813-61f1dae1fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:47:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "       Animal Control and Welfare Department       1.00      1.00      1.00       106\n",
      "         Electricity Distribution Department       1.00      1.00      1.00        99\n",
      "         Environmental Protection Department       1.00      1.00      1.00       101\n",
      "       Municipal Waste Management Department       1.00      1.00      1.00       105\n",
      "                     Public Works Department       1.00      1.00      1.00       107\n",
      "            Sewerage and Drainage Department       1.00      1.00      1.00        91\n",
      "       Traffic and Transportation Department       1.00      1.00      1.00        96\n",
      "   Urban Forestry and Landscaping Department       1.00      1.00      1.00        97\n",
      "Urban Lighting and Infrastructure Department       1.00      1.00      1.00        97\n",
      "      Water Supply and Sanitation Department       1.00      1.00      1.00       101\n",
      "\n",
      "                                    accuracy                           1.00      1000\n",
      "                                   macro avg       1.00      1.00      1.00      1000\n",
      "                                weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "âœ”ï¸ Department classifier trained, emphasized on 'type', and saved as .pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "df = df[['complaint_text', 'type', 'area', 'department']].dropna()\n",
    "\n",
    "# === Clean text to improve model's understanding ===\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text))  # Remove special chars\n",
    "    text = text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "df['complaint_text'] = df['complaint_text'].apply(clean_text)\n",
    "\n",
    "# === Encode target ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['department_encoded'] = label_encoder.fit_transform(df['department'])\n",
    "\n",
    "# === Define features and target ===\n",
    "X = df[['complaint_text', 'type', 'area']]\n",
    "y = df['department_encoded']\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Strongly emphasize 'type' by replicating feature ===\n",
    "X_train['type_emphasis'] = X_train['type']\n",
    "X_test['type_emphasis'] = X_test['type']\n",
    "\n",
    "# === Preprocessing ===\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=7000, ngram_range=(1, 3), stop_words='english'), 'complaint_text'),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), ['type', 'type_emphasis', 'area'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === XGBoost Model ===\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# === Full Pipeline ===\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_model)\n",
    "])\n",
    "\n",
    "# === Train the pipeline ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# === Save model and encoder ===\n",
    "joblib.dump(pipeline, 'department_classifier.pkl')\n",
    "joblib.dump(label_encoder, 'department_label_encoder.pkl')\n",
    "\n",
    "print(\"âœ”ï¸ Department classifier trained, emphasized on 'type', and saved as .pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed084cee-0d9d-4d0d-be34-7b5da0925781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Accuracy: 100.00%\n",
      "\n",
      "ðŸ” Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       495\n",
      "         Low       1.00      1.00      1.00       207\n",
      "      Medium       1.00      1.00      1.00       298\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['newpriority_label_encoder.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# === Load Data ===\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# === Filter & Clean ===\n",
    "df = df[['complaint_text', 'type', 'area', 'predicted_priority']].dropna()\n",
    "\n",
    "# Encode target\n",
    "label_encoder = LabelEncoder()\n",
    "df['priority_encoded'] = label_encoder.fit_transform(df['predicted_priority'])\n",
    "\n",
    "# Features & Target\n",
    "X = df[['complaint_text', 'type', 'area']]\n",
    "y = df['priority_encoded']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Preprocessing ===\n",
    "text_feature = 'complaint_text'\n",
    "cat_features = ['type', 'area']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), text_feature),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === Model ===\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    n_estimators=250,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# === Full Pipeline ===\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# === Training ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"ðŸ” Accuracy: {acc*100:.2f}%\\n\")\n",
    "print(\"ðŸ” Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# === Save Model ===\n",
    "joblib.dump(pipeline, 'newpriority_predictor.pkl')\n",
    "joblib.dump(label_encoder, 'newpriority_label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e80f777-a0d8-40b8-8c65-f875882dc658",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'final_complaints.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_priority\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(priority_mapping)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Save the updated CSV\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_complaints.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'final_complaints.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# Mapping for department\n",
    "department_mapping = {\n",
    "    \"Garbage Issue\": \"Municipal Waste Management Department\",\n",
    "    \"Electricity Issue\": \"Electricity Distribution Department\",\n",
    "    \"Illegal Parking\": \"Traffic and Transportation Department\",\n",
    "    \"Tree Falling\": \"Urban Forestry and Landscaping Department\",\n",
    "    \"Road Damage\": \"Public Works Department\",\n",
    "    \"Animal Nuisance\": \"Animal Control and Welfare Department\",\n",
    "    \"Streetlight Issue\": \"Urban Lighting and Infrastructure Department\",\n",
    "    \"Noise Pollution\": \"Environmental Protection Department\",\n",
    "    \"Water Issue\": \"Water Supply and Sanitation Department\",\n",
    "    \"Sewage Problem\": \"Sewerage and Drainage Department\"\n",
    "}\n",
    "\n",
    "# Mapping for predicted priority\n",
    "priority_mapping = {\n",
    "    \"Garbage Issue\": \"Medium\",\n",
    "    \"Electricity Issue\": \"High\",\n",
    "    \"Illegal Parking\": \"Medium\",\n",
    "    \"Tree Falling\": \"High\",\n",
    "    \"Road Damage\": \"High\",\n",
    "    \"Animal Nuisance\": \"Low\",\n",
    "    \"Streetlight Issue\": \"Medium\",\n",
    "    \"Noise Pollution\": \"Low\",\n",
    "    \"Water Issue\": \"High\",\n",
    "    \"Sewage Problem\": \"High\"\n",
    "}\n",
    "\n",
    "# Apply mappings to update the columns\n",
    "df['department'] = df['type'].map(department_mapping)\n",
    "df['predicted_priority'] = df['type'].map(priority_mapping)\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv('final_complaints.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d38d3b-871f-4cf5-a19d-f348cee728f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'resolved_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'resolved_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure the date columns are in datetime format\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiling_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiling_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresolved_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresolved_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Handle missing values (if any)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiling_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresolved_date\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'resolved_date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# Ensure the date columns are in datetime format\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')\n",
    "df['resolved_date'] = pd.to_datetime(df['resolved_date'], errors='coerce')\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df.dropna(subset=['filing_date', 'resolved_date'], inplace=True)\n",
    "\n",
    "# Create the target variable: Number of days to resolve the complaint\n",
    "df['resolved_in_days'] = (df['resolved_date'] - df['filing_date']).dt.days\n",
    "\n",
    "# === Feature Engineering ===\n",
    "# Extract features from the filing date (optional)\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_weekday'] = df['filing_date'].dt.weekday\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "df['type_encoded'] = label_encoder.fit_transform(df['type'])\n",
    "df['department_encoded'] = label_encoder.fit_transform(df['department'])\n",
    "df['predicted_priority_encoded'] = label_encoder.fit_transform(df['predicted_priority'])\n",
    "df['area_encoded'] = label_encoder.fit_transform(df['area'])\n",
    "\n",
    "# === Features & Target ===\n",
    "X = df[['area_encoded', 'filing_day', 'filing_month', 'filing_weekday', 'type_encoded', 'department_encoded', 'predicted_priority_encoded']]\n",
    "y = df['resolved_in_days']  # Target: Number of days to resolve the complaint\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Model: Random Forest Regressor ===\n",
    "model = Pipeline([\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"ðŸ” Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"ðŸ” RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# === Save the Model ===\n",
    "joblib.dump(model, 'eta_predictor.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e074074a-11b6-4765-8122-94bf65cd91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['complaint_id', 'complaint_text', 'area', 'type', 'filing_date',\n",
      "       'department', 'predicted_priority', 'status', 'area_eta', 'type_eta',\n",
      "       'resolved_days', 'resolution_date', 'predicted_priority_eta', 'rating',\n",
      "       'feedback_text', 'resolution_status', 'actual_cost', 'resolved_on',\n",
      "       'predicted_cost', 'actual_cost_anomaly', 'variance_flag',\n",
      "       'type_anomaly', 'area_anomaly', 'resolved_days_new',\n",
      "       'resolution_date_new', 'cost_difference', 'cost_diff_percent'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2efd6e12-c9fc-4c48-a3a7-2ff737119f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between 'resolved_days_new' and 'days_to_resolution' is: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "complaints_df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# Convert 'filing_date' and 'resolution_date_new' to datetime if not already in datetime format\n",
    "complaints_df['filing_date'] = pd.to_datetime(complaints_df['filing_date'], errors='coerce')\n",
    "complaints_df['resolution_date_new'] = pd.to_datetime(complaints_df['resolution_date_new'], errors='coerce')\n",
    "\n",
    "# Calculate the number of days between 'filing_date' and 'resolution_date_new'\n",
    "complaints_df['days_to_resolution'] = (complaints_df['resolution_date_new'] - complaints_df['filing_date']).dt.days\n",
    "\n",
    "# Now calculate the correlation between 'resolved_days_new' and 'days_to_resolution'\n",
    "correlation = complaints_df['resolved_days_new'].corr(complaints_df['days_to_resolution'])\n",
    "\n",
    "print(f\"The correlation between 'resolved_days_new' and 'days_to_resolution' is: {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125622a3-dec7-445d-8738-68ad0b28316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filing_date  resolved_days_new resolution_date_new\n",
      "0  2024-01-13                 16          2024-01-29\n",
      "1  2024-02-28                  0          2024-02-28\n",
      "2  2024-02-03                 14          2024-02-17\n",
      "3  2024-04-09                  0          2024-04-09\n",
      "4  2024-01-20                  0          2024-01-20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is loaded as complaints_df\n",
    "\n",
    "# Convert 'filing_date' to datetime if it's not already in datetime format\n",
    "complaints_df['filing_date'] = pd.to_datetime(complaints_df['filing_date'], errors='coerce')\n",
    "\n",
    "# Calculate the 'resolution_date_new' by adding 'resolved_days_new' to 'filing_date'\n",
    "complaints_df['resolution_date_new'] = complaints_df['filing_date'] + pd.to_timedelta(complaints_df['resolved_days_new'], unit='D')\n",
    "\n",
    "# Save the updated dataframe to a new CSV file (or overwrite the existing one)\n",
    "complaints_df.to_csv('final_complaints.csv', index=False)\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(complaints_df[['filing_date', 'resolved_days_new', 'resolution_date_new']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91eaf685-530d-427a-a5be-6437b934e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filing_date  resolved_days_new resolution_date_new\n",
      "0  2024-01-13                 16          2024-01-29\n",
      "1  2024-02-28                  0                 NaT\n",
      "2  2024-02-03                 14          2024-02-17\n",
      "3  2024-04-09                  0                 NaT\n",
      "4  2024-01-20                  0                 NaT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is loaded as complaints_df\n",
    "\n",
    "# Convert 'filing_date' to datetime if it's not already in datetime format\n",
    "complaints_df['filing_date'] = pd.to_datetime(complaints_df['filing_date'], errors='coerce')\n",
    "\n",
    "# Apply the condition: if 'resolved_days_new' is 0, set 'resolution_date_new' as NaT (empty)\n",
    "complaints_df['resolution_date_new'] = complaints_df.apply(\n",
    "    lambda row: row['filing_date'] + pd.to_timedelta(row['resolved_days_new'], unit='D') \n",
    "    if row['resolved_days_new'] > 0 else pd.NaT, axis=1\n",
    ")\n",
    "\n",
    "# Save the updated dataframe to a new CSV file (or overwrite the existing one)\n",
    "complaints_df.to_csv('final_complaints.csv', index=False)\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(complaints_df[['filing_date', 'resolved_days_new', 'resolution_date_new']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d9e6b5d-01fc-47b7-b218-962c58550d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved Date Model Evaluation:\n",
      "MAE: 36817386979869.19\n",
      "MSE: 1.5992461924486187e+27\n",
      "RMSE: 39990576295530.16\n",
      "\n",
      "Resolved Days Model Evaluation:\n",
      "MAE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # For RMSE calculation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load Data\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "# Ensure the date columns are in datetime format\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')\n",
    "\n",
    "# If 'resolved_date' column is not available, create it based on your business logic\n",
    "default_resolution_days = 7\n",
    "df['resolved_date'] = df['filing_date'] + pd.to_timedelta(default_resolution_days, unit='D')\n",
    "\n",
    "# Create 'resolved_days' as the number of days between 'filing_date' and 'resolved_date'\n",
    "df['resolved_days'] = (df['resolved_date'] - df['filing_date']).dt.days\n",
    "\n",
    "# Extract date features\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_weekday'] = df['filing_date'].dt.weekday\n",
    "\n",
    "# Encode categorical features using pre-trained LabelEncoders\n",
    "le_area = joblib.load('labelencoder_area.pkl')\n",
    "df['area_encoded'] = le_area.transform(df['area'])\n",
    "\n",
    "le_type = joblib.load('labelencoder_type.pkl')\n",
    "df['type_encoded'] = le_type.transform(df['type'])\n",
    "\n",
    "le_department = joblib.load('labelencoder_department.pkl')\n",
    "df['department_encoded'] = le_department.transform(df['department'])\n",
    "\n",
    "le_priority = joblib.load('labelencoder_priority.pkl')\n",
    "df['predicted_priority_encoded'] = le_priority.transform(df['predicted_priority'])\n",
    "\n",
    "# Step 3: Define Features and Targets\n",
    "X = df[['area_encoded', 'filing_day', 'filing_month', 'filing_weekday', \n",
    "        'type_encoded', 'department_encoded', 'predicted_priority_encoded']]\n",
    "\n",
    "y_resolved_date = df['resolved_date']\n",
    "y_resolved_days = df['resolved_days']\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_resolved_date_train, y_resolved_date_test = train_test_split(X, y_resolved_date, test_size=0.2, random_state=42)\n",
    "X_train2, X_test2, y_resolved_days_train, y_resolved_days_test = train_test_split(X, y_resolved_days, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Models\n",
    "model_resolved_date = LinearRegression()\n",
    "model_resolved_date.fit(X_train, y_resolved_date_train)\n",
    "\n",
    "model_resolved_days = LinearRegression()\n",
    "model_resolved_days.fit(X_train2, y_resolved_days_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "# Evaluate Resolved Date Model\n",
    "y_resolved_date_pred = model_resolved_date.predict(X_test)\n",
    "mae_resolved_date = mean_absolute_error(y_resolved_date_test, y_resolved_date_pred)\n",
    "mse_resolved_date = mean_squared_error(y_resolved_date_test, y_resolved_date_pred)\n",
    "rmse_resolved_date = np.sqrt(mse_resolved_date)\n",
    "\n",
    "# Evaluate Resolved Days Model\n",
    "y_resolved_days_pred = model_resolved_days.predict(X_test2)\n",
    "mae_resolved_days = mean_absolute_error(y_resolved_days_test, y_resolved_days_pred)\n",
    "mse_resolved_days = mean_squared_error(y_resolved_days_test, y_resolved_days_pred)\n",
    "rmse_resolved_days = np.sqrt(mse_resolved_days)\n",
    "\n",
    "# Step 7: Print Evaluation Metrics\n",
    "print(\"Resolved Date Model Evaluation:\")\n",
    "print(f\"MAE: {mae_resolved_date}\")\n",
    "print(f\"MSE: {mse_resolved_date}\")\n",
    "print(f\"RMSE: {rmse_resolved_date}\")\n",
    "\n",
    "print(\"\\nResolved Days Model Evaluation:\")\n",
    "print(f\"MAE: {mae_resolved_days}\")\n",
    "print(f\"MSE: {mse_resolved_days}\")\n",
    "print(f\"RMSE: {rmse_resolved_days}\")\n",
    "\n",
    "# Step 8: Save Models\n",
    "joblib.dump(model_resolved_date, 'resolved_date_predictor.pkl')\n",
    "joblib.dump(model_resolved_days, 'resolved_days_predictor.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fffb73d-866f-433c-b6dc-2dc34c0e7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Recreate or load label encoder for 'area'\n",
    "if os.path.exists('labelencoder_area.pkl'):\n",
    "    le_area = joblib.load('labelencoder_area.pkl')\n",
    "else:\n",
    "    le_area = LabelEncoder()\n",
    "    df['area_encoded'] = le_area.fit_transform(df['area'])\n",
    "    joblib.dump(le_area, 'labelencoder_area.pkl')\n",
    "\n",
    "# Recreate or load label encoder for 'type'\n",
    "if os.path.exists('labelencoder_type.pkl'):\n",
    "    le_type = joblib.load('labelencoder_type.pkl')\n",
    "else:\n",
    "    le_type = LabelEncoder()\n",
    "    df['type_encoded'] = le_type.fit_transform(df['type'])\n",
    "    joblib.dump(le_type, 'labelencoder_type.pkl')\n",
    "\n",
    "# Recreate or load label encoder for 'department'\n",
    "if os.path.exists('labelencoder_department.pkl'):\n",
    "    le_department = joblib.load('labelencoder_department.pkl')\n",
    "else:\n",
    "    le_department = LabelEncoder()\n",
    "    df['department_encoded'] = le_department.fit_transform(df['department'])\n",
    "    joblib.dump(le_department, 'labelencoder_department.pkl')\n",
    "\n",
    "# Recreate or load label encoder for 'predicted_priority'\n",
    "if os.path.exists('labelencoder_priority.pkl'):\n",
    "    le_priority = joblib.load('labelencoder_priority.pkl')\n",
    "else:\n",
    "    le_priority = LabelEncoder()\n",
    "    df['predicted_priority_encoded'] = le_priority.fit_transform(df['predicted_priority'])\n",
    "    joblib.dump(le_priority, 'labelencoder_priority.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "977e50a1-ab2a-4800-ae61-821d52f70a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved Date Model Evaluation:\n",
      "MAE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "\n",
      "Resolved Days Model Evaluation:\n",
      "MAE: 0.0\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "\n",
      "Cross-validation Results (Resolved Date):\n",
      "CV MAE: -0.0\n",
      "\n",
      "Cross-validation Results (Resolved Days):\n",
      "CV MAE: -0.0\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load Data\n",
    "df = pd.read_csv('final_complaints.csv')\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "# Ensure the date columns are in datetime format\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')\n",
    "\n",
    "# If 'resolved_date' column is not available, create it based on your business logic\n",
    "# Assuming default resolution time is 7 days for complaints\n",
    "default_resolution_days = 7  # Customize this duration based on business rules\n",
    "df['resolved_date'] = df['filing_date'] + pd.to_timedelta(default_resolution_days, unit='D')\n",
    "\n",
    "# Create 'resolved_days' as the number of days between 'filing_date' and 'resolved_date'\n",
    "df['resolved_days'] = (df['resolved_date'] - df['filing_date']).dt.days\n",
    "\n",
    "# Extract additional features from the filing_date\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_weekday'] = df['filing_date'].dt.weekday\n",
    "\n",
    "# Encode categorical features using pre-trained LabelEncoders\n",
    "le_area = joblib.load('labelencoder_area.pkl')  # Load the existing label encoder for 'area'\n",
    "df['area_encoded'] = le_area.transform(df['area'])\n",
    "\n",
    "le_type = joblib.load('labelencoder_type.pkl')  # Load the existing label encoder for 'type'\n",
    "df['type_encoded'] = le_type.transform(df['type'])\n",
    "\n",
    "le_department = joblib.load('labelencoder_department.pkl')  # Load the existing label encoder for 'department'\n",
    "df['department_encoded'] = le_department.transform(df['department'])\n",
    "\n",
    "le_priority = joblib.load('labelencoder_priority.pkl')  # Load the existing label encoder for 'predicted_priority'\n",
    "df['predicted_priority_encoded'] = le_priority.transform(df['predicted_priority'])\n",
    "\n",
    "# Step 3: Define Features and Targets\n",
    "# Define Features (independent variables)\n",
    "X = df[['area_encoded', 'filing_day', 'filing_month', 'filing_weekday', \n",
    "        'type_encoded', 'department_encoded', 'predicted_priority_encoded']]\n",
    "\n",
    "# Define Targets (dependent variables)\n",
    "y_resolved_date = df['resolved_date']  # Target: Exact resolved date\n",
    "y_resolved_days = df['resolved_days']  # Target: Number of days taken to resolve\n",
    "\n",
    "# Step 4: Train-Test Split for both models\n",
    "X_train, X_test, y_resolved_date_train, y_resolved_date_test = train_test_split(X, y_resolved_date, test_size=0.2, random_state=42)\n",
    "X_train2, X_test2, y_resolved_days_train, y_resolved_days_test = train_test_split(X, y_resolved_days, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Models\n",
    "# Model 1: Random Forest for resolving date\n",
    "model_resolved_date = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_resolved_date.fit(X_train, y_resolved_date_train)\n",
    "\n",
    "# Model 2: Random Forest for resolving days\n",
    "model_resolved_days = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_resolved_days.fit(X_train2, y_resolved_days_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "# Evaluate the resolved date model\n",
    "y_resolved_date_pred = model_resolved_date.predict(X_test)\n",
    "mae_resolved_date = mean_absolute_error(y_resolved_date_test, y_resolved_date_pred)\n",
    "mse_resolved_date = mean_squared_error(y_resolved_date_test, y_resolved_date_pred)\n",
    "rmse_resolved_date = mse_resolved_date ** 0.5  # Manually calculating RMSE\n",
    "\n",
    "# Evaluate the resolved days model\n",
    "y_resolved_days_pred = model_resolved_days.predict(X_test2)\n",
    "mae_resolved_days = mean_absolute_error(y_resolved_days_test, y_resolved_days_pred)\n",
    "mse_resolved_days = mean_squared_error(y_resolved_days_test, y_resolved_days_pred)\n",
    "rmse_resolved_days = mse_resolved_days ** 0.5  # Manually calculating RMSE\n",
    "\n",
    "print(f\"Resolved Date Model Evaluation:\")\n",
    "print(f\"MAE: {mae_resolved_date}\")\n",
    "print(f\"MSE: {mse_resolved_date}\")\n",
    "print(f\"RMSE: {rmse_resolved_date}\")\n",
    "\n",
    "print(f\"\\nResolved Days Model Evaluation:\")\n",
    "print(f\"MAE: {mae_resolved_days}\")\n",
    "print(f\"MSE: {mse_resolved_days}\")\n",
    "print(f\"RMSE: {rmse_resolved_days}\")\n",
    "\n",
    "# Step 7: Cross-validation to avoid overfitting\n",
    "cv_scores_resolved_date = cross_val_score(model_resolved_date, X, y_resolved_date, cv=5, scoring='neg_mean_absolute_error')\n",
    "cv_scores_resolved_days = cross_val_score(model_resolved_days, X, y_resolved_days, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(f\"\\nCross-validation Results (Resolved Date):\")\n",
    "print(f\"CV MAE: {-cv_scores_resolved_date.mean()}\")\n",
    "\n",
    "print(f\"\\nCross-validation Results (Resolved Days):\")\n",
    "print(f\"CV MAE: {-cv_scores_resolved_days.mean()}\")\n",
    "\n",
    "# Step 8: Save Models\n",
    "joblib.dump(model_resolved_date, 'resolved_date_predictor.pkl')\n",
    "joblib.dump(model_resolved_days, 'resolved_days_predictor.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82419885-0f1a-4a40-bdb7-e403a88e5fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Resolved Date Test Values:\n",
      " 1501   2024-01-14\n",
      "2586   2024-02-03\n",
      "2653   2024-02-29\n",
      "1055   2024-03-30\n",
      "705    2024-03-19\n",
      "Name: resolved_date, dtype: datetime64[ns]\n",
      "Predicted Resolved Date Values:\n",
      " [1.7051904e+18 1.7069184e+18 1.7091648e+18 1.7117568e+18 1.7108064e+18]\n"
     ]
    }
   ],
   "source": [
    "print(\"True Resolved Date Test Values:\\n\", y_resolved_date_test.head())\n",
    "print(\"Predicted Resolved Date Values:\\n\", y_resolved_date_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34fdf98d-f17b-420a-9e14-2706b7000ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable (y_resolved_date): 0   2024-01-20\n",
      "1   2024-03-06\n",
      "2   2024-02-10\n",
      "3   2024-04-16\n",
      "4   2024-01-27\n",
      "Name: resolved_date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Variable (y_resolved_date):\", y_resolved_date.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33ba7d4b-718d-48f5-9684-60abc5479fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaN in target variables: resolved_date    0\n",
      "resolved_days    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for NaN in target variables:\", df[['resolved_date', 'resolved_days']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afdde343-24fb-4cc3-92ce-fab794ca393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation between days_between_filing_resolution and resolution_date_new: 0.9999999999999998\n",
      "Chi-Square Test for type_encoded vs resolution_date_new:\n",
      "Chi2 Statistic: 210.26215123767219, P-value: 0.9739849059565284\n",
      "\n",
      "Chi-Square Test for area_encoded vs resolution_date_new:\n",
      "Chi2 Statistic: 818.0152022738928, P-value: 0.434302144212549\n",
      "\n",
      "Chi-Square Test for department_encoded vs resolution_date_new:\n",
      "Chi2 Statistic: 210.26215123767216, P-value: 0.9739849059565284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Sample data loading (replace with actual data)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Convert date columns to datetime if they are not already\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "\n",
    "# Create a new column for the difference in days between filing_date and resolution_date_new\n",
    "df['days_between_filing_resolution'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# Encode categorical variables (type, area, department)\n",
    "label_encoder_type = LabelEncoder()\n",
    "df['type_encoded'] = label_encoder_type.fit_transform(df['type'])\n",
    "\n",
    "label_encoder_area = LabelEncoder()\n",
    "df['area_encoded'] = label_encoder_area.fit_transform(df['area'])\n",
    "\n",
    "label_encoder_department = LabelEncoder()\n",
    "df['department_encoded'] = label_encoder_department.fit_transform(df['department'])\n",
    "\n",
    "# Now, let's check the correlation of categorical features (encoded) with the target variable 'resolution_date_new'\n",
    "# We will calculate Pearson correlation for numeric columns and Chi-square for categorical ones\n",
    "\n",
    "# Pearson correlation for numeric columns\n",
    "numeric_columns = ['days_between_filing_resolution']  # As filing_date and resolution_date_new are dates, we use the days difference\n",
    "for column in numeric_columns:\n",
    "    correlation = df[column].corr(df['days_between_filing_resolution'])\n",
    "    print(f\"Pearson Correlation between {column} and resolution_date_new: {correlation}\")\n",
    "\n",
    "# Chi-square test for categorical variables vs target ('resolution_date_new')\n",
    "# For categorical features, we will perform a chi-square test on the contingency table between categorical variables and the target variable\n",
    "\n",
    "categorical_columns = ['type_encoded', 'area_encoded', 'department_encoded']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    # Create a contingency table between the categorical feature and the target variable\n",
    "    contingency_table = pd.crosstab(df[column], df['days_between_filing_resolution'])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Chi-Square Test for {column} vs resolution_date_new:\")\n",
    "    print(f\"Chi2 Statistic: {chi2}, P-value: {p}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d425de7-7b8a-494c-8e8a-77230ee2a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation between filing_date and resolution_date_new (Days Difference): 0.9999999999999998\n",
      "Chi-Square Test for type_encoded vs resolution_date_new (Days Difference):\n",
      "Chi2 Statistic: 210.26215123767219, P-value: 0.9739849059565284\n",
      "\n",
      "Chi-Square Test for area_encoded vs resolution_date_new (Days Difference):\n",
      "Chi2 Statistic: 818.0152022738928, P-value: 0.434302144212549\n",
      "\n",
      "Chi-Square Test for department_encoded vs resolution_date_new (Days Difference):\n",
      "Chi2 Statistic: 210.26215123767216, P-value: 0.9739849059565284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Assuming df is your dataframe and it already contains the necessary columns\n",
    "# df = pd.read_csv('your_data.csv')  # Load your dataset\n",
    "\n",
    "# Convert the date columns to datetime if not already\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "\n",
    "# Calculate the difference in days between 'filing_date' and 'resolution_date_new'\n",
    "df['days_between_filing_resolution'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# Encode categorical variables (type, area, department) using LabelEncoder\n",
    "label_encoder_type = LabelEncoder()\n",
    "df['type_encoded'] = label_encoder_type.fit_transform(df['type'])\n",
    "\n",
    "label_encoder_area = LabelEncoder()\n",
    "df['area_encoded'] = label_encoder_area.fit_transform(df['area'])\n",
    "\n",
    "label_encoder_department = LabelEncoder()\n",
    "df['department_encoded'] = label_encoder_department.fit_transform(df['department'])\n",
    "\n",
    "# Now, we will check the correlation for categorical and numerical variables with 'resolution_date_new'\n",
    "\n",
    "# 1. Pearson correlation for numerical features (filing_date and resolution_date_new)\n",
    "# Since 'filing_date' and 'resolution_date_new' are dates, we will use the difference in days\n",
    "\n",
    "# Pearson correlation between days difference and the target variable (resolution_date_new)\n",
    "correlation_filing_resolution = df['days_between_filing_resolution'].corr(df['days_between_filing_resolution'])\n",
    "print(f\"Pearson Correlation between filing_date and resolution_date_new (Days Difference): {correlation_filing_resolution}\")\n",
    "\n",
    "# 2. Chi-Square test for categorical features (type, area, department)\n",
    "categorical_columns = ['type_encoded', 'area_encoded', 'department_encoded']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    # Create a contingency table between the categorical feature and the target variable (days difference)\n",
    "    contingency_table = pd.crosstab(df[column], df['days_between_filing_resolution'])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Chi-Square Test for {column} vs resolution_date_new (Days Difference):\")\n",
    "    print(f\"Chi2 Statistic: {chi2}, P-value: {p}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f515c94-9c55-421f-a4e7-6bedc708877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "MAE: 570659506206853.1\n",
      "MSE: 4.601564749587255e+31\n",
      "RMSE: 6783483433743503.0\n",
      "MAPE: 6.819904699052829e-05\n",
      "Model saved successfully as 'ETA_predictor.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import joblib\n",
    "\n",
    "# 1. Load the data (Assume df is already loaded with required columns)\n",
    "# df = pd.read_csv('your_data.csv') # Un-comment and modify this to load your data\n",
    "\n",
    "# 2. Convert 'filing_date' and 'resolution_date_new' to datetime if they are not\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "\n",
    "# 3. Calculate the target variable (days between filing_date and resolution_date_new)\n",
    "df['resolved_days_new'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# 4. Prepare features (filing_date, resolved_days_new) and target (resolution_date_new)\n",
    "df['filing_date_numeric'] = df['filing_date'].astype(np.int64) // 10**9  # Convert filing_date to seconds\n",
    "X = df[['filing_date_numeric', 'resolved_days_new']]  # Features\n",
    "y = df['resolution_date_new']  # Target\n",
    "\n",
    "# 5. Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Train a RandomForestRegressor model (or any other model you prefer)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 8. Model evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Evaluation:\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "\n",
    "# 9. Save the trained model to a file using joblib\n",
    "joblib.dump(model, 'ETA_predictor.pkl')\n",
    "\n",
    "print(\"Model saved successfully as 'ETA_predictor.pkl'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96050518-3e51-4007-898e-66bf00d7b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Fit and transform your features\n",
    "\n",
    "# Use scaled features for training and evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1817fc5b-b0b6-4c69-9114-3e39fb480098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b94c8787-ba7f-4bc1-a18b-44e8f7a698ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_year'] = df['filing_date'].dt.year\n",
    "df['filing_day_of_week'] = df['filing_date'].dt.weekday\n",
    "df['filing_quarter'] = df['filing_date'].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7115d0b-8951-48a7-8cbb-c6f1b0843f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing dates with a default date (e.g., the first day of the year)\n",
    "df['filing_date'].fillna(pd.to_datetime('2024-01-01'), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8314920b-fbad-40bc-bb2f-a30be9820f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['filing_day', 'filing_month', 'filing_year', 'filing_day_of_week', 'filing_quarter', 'resolved_days_new']]\n",
    "y = df['resolution_date_new']  # Target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac49eca0-980e-4501-bc41-d2ca271f15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7394f896-b2c4-41f4-af77-d24cceea1124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9aa6e018-b9db-491c-a55e-02b2de6ae887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.72 53.28 47.57]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example DataFrame (replace with your data)\n",
    "df = pd.DataFrame({\n",
    "    'filing_date': ['2024-01-01', '2024-02-15', '2024-03-10'],\n",
    "    'resolved_days_new': [30, 60, 45],\n",
    "    'resolution_date_new': ['2024-02-01', '2024-04-15', '2024-04-25']\n",
    "})\n",
    "\n",
    "# Convert filing_date to datetime\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "\n",
    "# Extract day, month, year, weekday, and quarter from filing_date\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_year'] = df['filing_date'].dt.year\n",
    "df['filing_day_of_week'] = df['filing_date'].dt.weekday\n",
    "df['filing_quarter'] = df['filing_date'].dt.quarter\n",
    "\n",
    "# Target variable (convert resolution_date_new to days)\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "df['days_to_resolution'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df[['filing_day', 'filing_month', 'filing_year', 'filing_day_of_week', 'filing_quarter', 'resolved_days_new']]\n",
    "y = df['days_to_resolution']  # Target is the number of days to resolution\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(model, 'ETA_predictor.pkl')\n",
    "\n",
    "# Check the model's performance\n",
    "predictions = model.predict(X_scaled)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818ff88-af59-42a7-808a-600f78ea528c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e913cdd0-77f6-4302-bd4e-dcaa5ef5eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Model Evaluation (Sharply Tuned):\n",
      "MAE  : 23.68\n",
      "MSE  : 560.74\n",
      "RMSE : 23.68\n",
      "MAPE : 0.7639\n",
      "\n",
      "âœ… Optimized Model saved as 'ETA_days_predictor_optimized.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------- Load & Prepare Data ---------------------\n",
    "# df = pd.read_csv(\"your_data.csv\")  # Uncomment if using CSV\n",
    "\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "df['resolved_days_new'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# -------------------- Feature Engineering ---------------------\n",
    "df['filing_date_numeric'] = df['filing_date'].astype(np.int64) // 10**9\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_weekday'] = df['filing_date'].dt.weekday\n",
    "\n",
    "# Features and Target\n",
    "X = df[['filing_date_numeric', 'filing_day', 'filing_month', 'filing_weekday']]\n",
    "y = df['resolved_days_new']\n",
    "\n",
    "# -------------------- Train-Test Split ---------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- Smart CV Split ---------------------\n",
    "n_samples = len(X_train)\n",
    "cv_splits = min(5, n_samples if n_samples > 1 else 2)\n",
    "cv = KFold(n_splits=cv_splits)\n",
    "\n",
    "# -------------------- Hyperparameter Tuning ---------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# -------------------- Evaluation ---------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nðŸ” Model Evaluation (Sharply Tuned):\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"MAPE : {mape:.4f}\")\n",
    "\n",
    "# -------------------- Save Model ---------------------\n",
    "joblib.dump(best_model, 'ETA_days_predictor_optimized.pkl')\n",
    "print(\"\\nâœ… Optimized Model saved as 'ETA_days_predictor_optimized.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1749a1fc-5b7b-44c6-addd-b903842bf0ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n120 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1387, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1397, in _check_y\n    y = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input y contains NaN.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 61\u001b[0m\n\u001b[0;32m     48\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     53\u001b[0m }\n\u001b[0;32m     54\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     55\u001b[0m     RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     56\u001b[0m     param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m )\n\u001b[1;32m---> 61\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# -------------------- Evaluate ---------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n120 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1387, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1397, in _check_y\n    y = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input y contains NaN.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------- Load Data ---------------------\n",
    "df = pd.read_csv('final_complaints_updated.csv')  # Uncomment if loading fresh\n",
    "\n",
    "# Convert date columns\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['resolution_date_new'] = pd.to_datetime(df['resolution_date_new'])\n",
    "\n",
    "# Calculate target\n",
    "df['resolved_days_new'] = (df['resolution_date_new'] - df['filing_date']).dt.days\n",
    "\n",
    "# -------------------- Feature Engineering ---------------------\n",
    "\n",
    "# Date-related features\n",
    "df['filing_date_numeric'] = df['filing_date'].astype(np.int64) // 10**9\n",
    "df['filing_day'] = df['filing_date'].dt.day\n",
    "df['filing_month'] = df['filing_date'].dt.month\n",
    "df['filing_weekday'] = df['filing_date'].dt.weekday\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in ['area', 'type', 'department', 'predicted_priority']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Save encoder for future use (if needed in deployment)\n",
    "\n",
    "# -------------------- Define Features and Target ---------------------\n",
    "features = [\n",
    "    'filing_date_numeric', 'filing_day', 'filing_month', 'filing_weekday',\n",
    "    'area', 'type', 'department', 'predicted_priority'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['resolved_days_new']\n",
    "\n",
    "# -------------------- Train-Test Split ---------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------- Hyperparameter Tuning ---------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# -------------------- Evaluate ---------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nðŸ” Model Evaluation (With More Features):\")\n",
    "print(f\"MAE  : {mae:.2f} days\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"MAPE : {mape:.4f}\")\n",
    "\n",
    "# -------------------- Save Model ---------------------\n",
    "joblib.dump(best_model, 'ETA_days_predictor_extended.pkl')\n",
    "print(\"\\nâœ… Improved Model saved as 'ETA_days_predictor_extended.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "359906de-621a-4a00-a350-b1347f9ba994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filing_date', 'resolved_days_new', 'resolution_date_new', 'filing_day', 'filing_month', 'filing_year', 'filing_day_of_week', 'filing_quarter', 'days_to_resolution', 'filing_date_numeric', 'filing_weekday']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42f83661-6d88-4e39-a668-901618f88f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'resolved_days_new' is NaN\n",
    "df = df.dropna(subset=['resolved_days_new'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "935ffdb7-71ec-4095-97c0-bc5985ec776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['resolved_days_new', 'resolution_date_new'])  # Drop target & unused date column\n",
    "y = df['resolved_days_new']\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "776ff94d-ab87-457b-85b9-b4646a2e7f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n24 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 973, in check_array\n    array = array.astype(new_dtype)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 430, in astype\n    return self.apply(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'C4074'\n\n--------------------------------------------------------------------------------\n96 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 973, in check_array\n    array = array.astype(new_dtype)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 430, in astype\n    return self.apply(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'C0337'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n24 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 973, in check_array\n    array = array.astype(new_dtype)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 430, in astype\n    return self.apply(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'C4074'\n\n--------------------------------------------------------------------------------\n96 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 973, in check_array\n    array = array.astype(new_dtype)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 430, in astype\n    return self.apply(\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"C:\\Users\\SAMYAK KHANDERAO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'C0337'\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5a68a59-b868-446a-aa5f-366e18c6f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['complaint_id', 'complaint_text', 'status', 'area_eta', 'type_eta',\n",
      "       'resolution_date', 'predicted_priority_eta', 'feedback_text',\n",
      "       'resolution_status', 'resolved_on', 'variance_flag', 'type_anomaly',\n",
      "       'area_anomaly'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns in the dataset\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "print(non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9058523d-9941-49d5-bdf0-be8d4fb70245",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'categorical_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'categorical_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example for encoding a categorical column\u001b[39;00m\n\u001b[0;32m      4\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m----> 5\u001b[0m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'categorical_column'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example for encoding a categorical column\n",
    "label_encoder = LabelEncoder()\n",
    "X['categorical_column'] = label_encoder.fit_transform(X['categorical_column'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370bc27-56db-43ea-9cc9-37a6fbb49959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
